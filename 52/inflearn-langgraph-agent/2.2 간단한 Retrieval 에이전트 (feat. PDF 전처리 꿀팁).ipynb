{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-05T05:53:21.117118Z",
     "start_time": "2025-10-05T05:53:20.283254Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file_path = './documents/income_tax.pdf'\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T05:53:21.843136Z",
     "start_time": "2025-10-05T05:53:21.839385Z"
    }
   },
   "cell_type": "code",
   "source": "pages[35]",
   "id": "286acbdc01a0678a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-01-10T10:55:01+09:00', 'moddate': '2025-01-10T10:55:01+09:00', 'source': './documents/income_tax.pdf', 'total_pages': 133, 'page': 35, 'page_label': '36'}, page_content='법제처                                                            36                                                       국가법령정보센터\\n소득세법 \\n② 거주자의 퇴직소득에 대한 소득세는 다음 각 호의 순서에 따라 계산한 금액(이하 “퇴직소득 산출세액”이라 한다\\n)으로 한다.<개정 2013. 1. 1., 2014. 12. 23.>\\n1. 해당 과세기간의 퇴직소득과세표준에 제1항의 세율을 적용하여 계산한 금액\\n2. 제1호의 금액을 12로 나눈 금액에 근속연수를 곱한 금액\\n3. 삭제<2014. 12. 23.>\\n[전문개정 2009. 12. 31.]\\n \\n           제2관 세액공제 <개정 2009. 12. 31.>\\n \\n제56조(배당세액공제) ① 거주자의 종합소득금액에 제17조제3항 각 호 외의 부분 단서가 적용되는 배당소득금액이 합\\n산되어 있는 경우에는 같은 항 각 호 외의 부분 단서에 따라 해당 과세기간의 총수입금액에 더한 금액에 해당하는\\n금액을 종합소득 산출세액에서 공제한다. <개정 2009. 12. 31.>\\n② 제1항에 따른 공제를 “배당세액공제”라 한다.<개정 2009. 12. 31.>\\n③ 삭제<2003. 12. 30.>\\n④ 제1항을 적용할 때 배당세액공제의 대상이 되는 배당소득금액은 제14조제2항의 종합소득과세표준에 포함된 배\\n당소득금액으로서 이자소득등의 종합과세기준금액을 초과하는 것으로 한다.<개정 2009. 12. 31.>\\n⑤ 삭제<2006. 12. 30.>\\n⑥ 배당세액공제액의 계산 등에 필요한 사항은 대통령령으로 정한다.<개정 2009. 12. 31.>\\n[제목개정 2009. 12. 31.]\\n \\n제56조의2(기장세액공제) ① 제160조제3항에 따른 간편장부대상자가 제70조 또는 제74조에 따른 과세표준확정신고를\\n할 때 복식부기에 따라 기장(記帳)하여 소득금액을 계산하고 제70조제4항제3호에 따른 서류를 제출하는 경우에는\\n해당 장부에 의하여 계산한 사업소득금액이 종합소득금액에서 차지하는 비율을 종합소득 산출세액에 곱하여 계산한\\n금액의 100분의 20에 해당하는 금액을 종합소득 산출세액에서 공제한다. 다만, 공제세액이 100만원을 초과하는 경\\n우에는 100만원을 공제한다.\\n② 다음 각 호의 어느 하나에 해당하는 경우에는 제1항에 따른 공제[이하 “기장세액공제”(記帳稅額控除)라 한다]를\\n적용하지 아니한다.\\n1. 비치ㆍ기록한 장부에 의하여 신고하여야 할 소득금액의 100분의 20 이상을 누락하여 신고한 경우\\n2. 기장세액공제와 관련된 장부 및 증명서류를 해당 과세표준확정신고기간 종료일부터 5년간 보관하지 아니한 경\\n우. 다만, 천재지변 등 대통령령으로 정하는 부득이한 사유에 해당하는 경우에는 그러하지 아니하다. \\n③ 기장세액공제에 관하여 필요한 사항은 대통령령으로 정한다.\\n[전문개정 2009. 12. 31.]\\n \\n제56조의3(전자계산서 발급 전송에 대한 세액공제) ① 총수입금액 등을 고려하여 대통령령으로 정하는 사업자가 제\\n163조제1항 후단에 따른 전자계산서를 2027년 12월 31일까지 발급(제163조제8항에 따라 전자계산서 발급명세를\\n국세청장에게 전송하는 경우로 한정한다)하는 경우에는 전자계산서 발급 건수 등을 고려하여 대통령령으로 정하는\\n금액을 해당 과세기간의 사업소득에 대한 종합소득산출세액에서 공제할 수 있다. 이 경우 공제한도는 연간 100만원\\n으로 한다. <개정 2021. 12. 8., 2024. 12. 31.>\\n② 제1항에 따른 세액공제를 적용받으려는 사업자는 제70조 또는 제74조에 따른 과세표준확정신고를 할 때 기획재\\n정부령으로 정하는 전자계산서 발급 세액공제신고서를 납세지 관할 세무서장에게 제출하여야 한다.\\n[본조신설 2014. 12. 23.]\\n \\n제57조(외국납부세액공제) ① 거주자의 종합소득금액 또는 퇴직소득금액에 국외원천소득이 합산되어 있는 경우로서\\n그 국외원천소득에 대하여 외국에서 대통령령으로 정하는 외국소득세액(이하 이 조에서 “외국소득세액”이라 한다)을\\n납부하였거나 납부할 것이 있을 때에는 다음 계산식에 따라 계산한 금액(이하 이 조에서 “공제한도금액”이라 한다)')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T23:53:55.300309Z",
     "start_time": "2025-09-30T23:53:55.298132Z"
    }
   },
   "cell_type": "code",
   "source": "print(pages[35].page_content)",
   "id": "147401956ff1825b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "법제처                                                            36                                                       국가법령정보센터\n",
      "소득세법 \n",
      "② 거주자의 퇴직소득에 대한 소득세는 다음 각 호의 순서에 따라 계산한 금액(이하 “퇴직소득 산출세액”이라 한다\n",
      ")으로 한다.<개정 2013. 1. 1., 2014. 12. 23.>\n",
      "1. 해당 과세기간의 퇴직소득과세표준에 제1항의 세율을 적용하여 계산한 금액\n",
      "2. 제1호의 금액을 12로 나눈 금액에 근속연수를 곱한 금액\n",
      "3. 삭제<2014. 12. 23.>\n",
      "[전문개정 2009. 12. 31.]\n",
      " \n",
      "           제2관 세액공제 <개정 2009. 12. 31.>\n",
      " \n",
      "제56조(배당세액공제) ① 거주자의 종합소득금액에 제17조제3항 각 호 외의 부분 단서가 적용되는 배당소득금액이 합\n",
      "산되어 있는 경우에는 같은 항 각 호 외의 부분 단서에 따라 해당 과세기간의 총수입금액에 더한 금액에 해당하는\n",
      "금액을 종합소득 산출세액에서 공제한다. <개정 2009. 12. 31.>\n",
      "② 제1항에 따른 공제를 “배당세액공제”라 한다.<개정 2009. 12. 31.>\n",
      "③ 삭제<2003. 12. 30.>\n",
      "④ 제1항을 적용할 때 배당세액공제의 대상이 되는 배당소득금액은 제14조제2항의 종합소득과세표준에 포함된 배\n",
      "당소득금액으로서 이자소득등의 종합과세기준금액을 초과하는 것으로 한다.<개정 2009. 12. 31.>\n",
      "⑤ 삭제<2006. 12. 30.>\n",
      "⑥ 배당세액공제액의 계산 등에 필요한 사항은 대통령령으로 정한다.<개정 2009. 12. 31.>\n",
      "[제목개정 2009. 12. 31.]\n",
      " \n",
      "제56조의2(기장세액공제) ① 제160조제3항에 따른 간편장부대상자가 제70조 또는 제74조에 따른 과세표준확정신고를\n",
      "할 때 복식부기에 따라 기장(記帳)하여 소득금액을 계산하고 제70조제4항제3호에 따른 서류를 제출하는 경우에는\n",
      "해당 장부에 의하여 계산한 사업소득금액이 종합소득금액에서 차지하는 비율을 종합소득 산출세액에 곱하여 계산한\n",
      "금액의 100분의 20에 해당하는 금액을 종합소득 산출세액에서 공제한다. 다만, 공제세액이 100만원을 초과하는 경\n",
      "우에는 100만원을 공제한다.\n",
      "② 다음 각 호의 어느 하나에 해당하는 경우에는 제1항에 따른 공제[이하 “기장세액공제”(記帳稅額控除)라 한다]를\n",
      "적용하지 아니한다.\n",
      "1. 비치ㆍ기록한 장부에 의하여 신고하여야 할 소득금액의 100분의 20 이상을 누락하여 신고한 경우\n",
      "2. 기장세액공제와 관련된 장부 및 증명서류를 해당 과세표준확정신고기간 종료일부터 5년간 보관하지 아니한 경\n",
      "우. 다만, 천재지변 등 대통령령으로 정하는 부득이한 사유에 해당하는 경우에는 그러하지 아니하다. \n",
      "③ 기장세액공제에 관하여 필요한 사항은 대통령령으로 정한다.\n",
      "[전문개정 2009. 12. 31.]\n",
      " \n",
      "제56조의3(전자계산서 발급 전송에 대한 세액공제) ① 총수입금액 등을 고려하여 대통령령으로 정하는 사업자가 제\n",
      "163조제1항 후단에 따른 전자계산서를 2027년 12월 31일까지 발급(제163조제8항에 따라 전자계산서 발급명세를\n",
      "국세청장에게 전송하는 경우로 한정한다)하는 경우에는 전자계산서 발급 건수 등을 고려하여 대통령령으로 정하는\n",
      "금액을 해당 과세기간의 사업소득에 대한 종합소득산출세액에서 공제할 수 있다. 이 경우 공제한도는 연간 100만원\n",
      "으로 한다. <개정 2021. 12. 8., 2024. 12. 31.>\n",
      "② 제1항에 따른 세액공제를 적용받으려는 사업자는 제70조 또는 제74조에 따른 과세표준확정신고를 할 때 기획재\n",
      "정부령으로 정하는 전자계산서 발급 세액공제신고서를 납세지 관할 세무서장에게 제출하여야 한다.\n",
      "[본조신설 2014. 12. 23.]\n",
      " \n",
      "제57조(외국납부세액공제) ① 거주자의 종합소득금액 또는 퇴직소득금액에 국외원천소득이 합산되어 있는 경우로서\n",
      "그 국외원천소득에 대하여 외국에서 대통령령으로 정하는 외국소득세액(이하 이 조에서 “외국소득세액”이라 한다)을\n",
      "납부하였거나 납부할 것이 있을 때에는 다음 계산식에 따라 계산한 금액(이하 이 조에서 “공제한도금액”이라 한다)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T05:54:43.821793Z",
     "start_time": "2025-10-05T05:54:43.813549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "134743bf5d1e3f2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[zerox](https://github.com/getomni-ai/zerox)\n",
    "\n",
    "- ocr, llm 을 이용해 문서 인식\n",
    "\n",
    "깃허브에 전처리된 내용 있으므로 아래 2 셀은 실제로 실행해볼 필요는 없음"
   ],
   "id": "35d8b99ef5809de5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:03:15.426267Z",
     "start_time": "2025-10-05T06:03:15.420853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "id": "7283bd3d56ca4efe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:12:32.209086Z",
     "start_time": "2025-10-05T06:11:06.624952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyzerox import zerox\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "### 모델 설정 (Vision 모델만 사용) 참고: https://docs.litellm.ai/docs/providers ###\n",
    "\n",
    "## 일부 모델에 필요할 수 있는 추가 모델 kwargs의 자리 표시자\n",
    "kwargs = {}\n",
    "\n",
    "## Vision 모델에 사용할 시스템 프롬프트\n",
    "custom_system_prompt = None\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "# 메인 비동기 진입점을 정의합니다\n",
    "async def main():\n",
    "    file_path = \"./documents/income_tax.pdf\" ## 로컬 파일 경로 및 파일 URL 지원\n",
    "\n",
    "    ## 일부 페이지 또는 전체 페이지를 처리\n",
    "    select_pages = None ## 전체는 None, 특정 페이지는 int 또는 list(int) 페이지 번호 (1부터 시작)\n",
    "\n",
    "    output_dir = \"./documents\" ## 통합된 마크다운 파일을 저장할 디렉토리\n",
    "    result = await zerox(\n",
    "        file_path=file_path,\n",
    "        model=model,\n",
    "        output_dir=output_dir,\n",
    "        custom_system_prompt=custom_system_prompt,\n",
    "        select_pages=select_pages,\n",
    "        **kwargs\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# 메인 함수를 실행합니다:\n",
    "result = asyncio.run(main())\n",
    "\n",
    "# 마크다운 결과를 출력합니다\n",
    "print(result)"
   ],
   "id": "5edae445680d18ef",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:823\u001B[39m, in \u001B[36mOpenAIChatCompletion.acompletion\u001B[39m\u001B[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001B[39m\n\u001B[32m    810\u001B[39m logging_obj.pre_call(\n\u001B[32m    811\u001B[39m     \u001B[38;5;28minput\u001B[39m=data[\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    812\u001B[39m     api_key=openai_aclient.api_key,\n\u001B[32m   (...)\u001B[39m\u001B[32m    820\u001B[39m     },\n\u001B[32m    821\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m headers, response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.make_openai_chat_completion_request(\n\u001B[32m    824\u001B[39m     openai_aclient=openai_aclient,\n\u001B[32m    825\u001B[39m     data=data,\n\u001B[32m    826\u001B[39m     timeout=timeout,\n\u001B[32m    827\u001B[39m     logging_obj=logging_obj,\n\u001B[32m    828\u001B[39m )\n\u001B[32m    829\u001B[39m stringified_response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001B[39m, in \u001B[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m func(*args, **kwargs)\n\u001B[32m    191\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:454\u001B[39m, in \u001B[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001B[39m\u001B[34m(self, openai_aclient, data, timeout, logging_obj)\u001B[39m\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m454\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:436\u001B[39m, in \u001B[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001B[39m\u001B[34m(self, openai_aclient, data, timeout, logging_obj)\u001B[39m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    435\u001B[39m     raw_response = (\n\u001B[32m--> \u001B[39m\u001B[32m436\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m openai_aclient.chat.completions.with_raw_response.create(\n\u001B[32m    437\u001B[39m             **data, timeout=timeout\n\u001B[32m    438\u001B[39m         )\n\u001B[32m    439\u001B[39m     )\n\u001B[32m    440\u001B[39m     end_time = time.time()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001B[39m, in \u001B[36masync_to_raw_response_wrapper.<locals>.wrapped\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    379\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mextra_headers\u001B[39m\u001B[33m\"\u001B[39m] = extra_headers\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], \u001B[38;5;28;01mawait\u001B[39;00m func(*args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2585\u001B[39m, in \u001B[36mAsyncCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   2584\u001B[39m validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m2585\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   2586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2587\u001B[39m     body=\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[32m   2588\u001B[39m         {\n\u001B[32m   2589\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m   2590\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   2591\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m   2592\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   2593\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m   2594\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m   2595\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m   2596\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m   2597\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m   2598\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m   2599\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   2600\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m   2601\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m   2602\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   2603\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m   2604\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m   2605\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   2606\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m   2607\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: response_format,\n\u001B[32m   2608\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   2609\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m   2610\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   2611\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m   2612\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   2613\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m   2614\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   2615\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   2616\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   2617\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   2618\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   2619\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   2620\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   2621\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: verbosity,\n\u001B[32m   2622\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m   2623\u001B[39m         },\n\u001B[32m   2624\u001B[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001B[32m   2625\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m   2626\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001B[32m   2627\u001B[39m     ),\n\u001B[32m   2628\u001B[39m     options=make_request_options(\n\u001B[32m   2629\u001B[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m   2630\u001B[39m     ),\n\u001B[32m   2631\u001B[39m     cast_to=ChatCompletion,\n\u001B[32m   2632\u001B[39m     stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   2633\u001B[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001B[32m   2634\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001B[39m, in \u001B[36mAsyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1791\u001B[39m opts = FinalRequestOptions.construct(\n\u001B[32m   1792\u001B[39m     method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), **options\n\u001B[32m   1793\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1794\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1594\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1593\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1594\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1596\u001B[39m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[31mRateLimitError\u001B[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mOpenAIError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/main.py:575\u001B[39m, in \u001B[36macompletion\u001B[39m\u001B[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, safety_identifier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001B[39m\n\u001B[32m    574\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m asyncio.iscoroutine(init_response):\n\u001B[32m--> \u001B[39m\u001B[32m575\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m init_response\n\u001B[32m    576\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:870\u001B[39m, in \u001B[36mOpenAIChatCompletion.acompletion\u001B[39m\u001B[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001B[39m\n\u001B[32m    868\u001B[39m message = \u001B[38;5;28mgetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mmessage\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(e))\n\u001B[32m--> \u001B[39m\u001B[32m870\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m OpenAIError(\n\u001B[32m    871\u001B[39m     status_code=status_code,\n\u001B[32m    872\u001B[39m     message=message,\n\u001B[32m    873\u001B[39m     headers=error_headers,\n\u001B[32m    874\u001B[39m     body=exception_body,\n\u001B[32m    875\u001B[39m )\n",
      "\u001B[31mOpenAIError\u001B[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/models/modellitellm.py:98\u001B[39m, in \u001B[36mlitellmmodel.completion\u001B[39m\u001B[34m(self, image_path, maintain_format, prior_page)\u001B[39m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m litellm.acompletion(model=\u001B[38;5;28mself\u001B[39m.model, messages=messages, **\u001B[38;5;28mself\u001B[39m.kwargs)\n\u001B[32m    100\u001B[39m     \u001B[38;5;66;03m## completion response\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/utils.py:1606\u001B[39m, in \u001B[36mclient.<locals>.wrapper_async\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1605\u001B[39m \u001B[38;5;28msetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m, timeout)\n\u001B[32m-> \u001B[39m\u001B[32m1606\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/utils.py:1457\u001B[39m, in \u001B[36mclient.<locals>.wrapper_async\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1456\u001B[39m \u001B[38;5;66;03m# MODEL CALL\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1457\u001B[39m result = \u001B[38;5;28;01mawait\u001B[39;00m original_function(*args, **kwargs)\n\u001B[32m   1458\u001B[39m end_time = datetime.datetime.now()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/main.py:594\u001B[39m, in \u001B[36macompletion\u001B[39m\u001B[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, safety_identifier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001B[39m\n\u001B[32m    593\u001B[39m custom_llm_provider = custom_llm_provider \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mopenai\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m594\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mexception_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    595\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    596\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    597\u001B[39m \u001B[43m    \u001B[49m\u001B[43moriginal_exception\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    598\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompletion_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompletion_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    599\u001B[39m \u001B[43m    \u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    600\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2273\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2272\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mlitellm_response_headers\u001B[39m\u001B[33m\"\u001B[39m, litellm_response_headers)\n\u001B[32m-> \u001B[39m\u001B[32m2273\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   2274\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:330\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m    329\u001B[39m     exception_mapping_worked = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m330\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m RateLimitError(\n\u001B[32m    331\u001B[39m         message=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRateLimitError: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexception_provider\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmessage\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    332\u001B[39m         model=model,\n\u001B[32m    333\u001B[39m         llm_provider=custom_llm_provider,\n\u001B[32m    334\u001B[39m         response=\u001B[38;5;28mgetattr\u001B[39m(original_exception, \u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    335\u001B[39m     )\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m ExceptionCheckers.is_error_str_context_window_exceeded(error_str):\n",
      "\u001B[31mRateLimitError\u001B[39m: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/processor/pdf.py:63\u001B[39m, in \u001B[36mprocess_page\u001B[39m\u001B[34m(image, model, temp_directory, input_token_count, output_token_count, prior_page, semaphore)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     completion = \u001B[38;5;28;01mawait\u001B[39;00m model.completion(\n\u001B[32m     64\u001B[39m         image_path=image_path,\n\u001B[32m     65\u001B[39m         maintain_format=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     66\u001B[39m         prior_page=prior_page,\n\u001B[32m     67\u001B[39m     )\n\u001B[32m     69\u001B[39m     formatted_markdown = format_markdown(completion.content)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/models/modellitellm.py:109\u001B[39m, in \u001B[36mlitellmmodel.completion\u001B[39m\u001B[34m(self, image_path, maintain_format, prior_page)\u001B[39m\n\u001B[32m    108\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m109\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(Messages.COMPLETION_ERROR.format(err))\n",
      "\u001B[31mException\u001B[39m: \n    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-RCF6KO2QXqIjRpKCywAH9Gu9 on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n    Please check the status of your model provider API status.\n    ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 36\u001B[39m\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# 메인 함수를 실행합니다:\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m result = \u001B[43masyncio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# 마크다운 결과를 출력합니다\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:35\u001B[39m, in \u001B[36m_patch_asyncio.<locals>.run\u001B[39m\u001B[34m(main, debug)\u001B[39m\n\u001B[32m     33\u001B[39m task.cancel()\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m suppress(asyncio.CancelledError):\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m     \u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:92\u001B[39m, in \u001B[36m_patch_loop.<locals>.run_until_complete\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m     90\u001B[39m     f._log_destroy_pending = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f.done():\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stopping:\n\u001B[32m     94\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:133\u001B[39m, in \u001B[36m_patch_loop.<locals>._run_once\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    130\u001B[39m curr_task = curr_tasks.pop(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m     \u001B[43mhandle\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    135\u001B[39m     \u001B[38;5;66;03m# restore the current task\u001B[39;00m\n\u001B[32m    136\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m curr_task \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py:88\u001B[39m, in \u001B[36mHandle._run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     87\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mSystemExit\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n\u001B[32m     90\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:388\u001B[39m, in \u001B[36mTask.__wakeup\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m    385\u001B[39m     future.result()\n\u001B[32m    386\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    387\u001B[39m     \u001B[38;5;66;03m# This may also be a cancellation.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    390\u001B[39m     \u001B[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001B[39;00m\n\u001B[32m    391\u001B[39m     \u001B[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    394\u001B[39m     \u001B[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001B[39;00m\n\u001B[32m    395\u001B[39m     \u001B[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001B[39;00m\n\u001B[32m    396\u001B[39m     \u001B[38;5;28mself\u001B[39m.__step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:303\u001B[39m, in \u001B[36mTask.__step\u001B[39m\u001B[34m(self, exc)\u001B[39m\n\u001B[32m    301\u001B[39m _enter_task(\u001B[38;5;28mself\u001B[39m._loop, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__step_run_and_handle_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    305\u001B[39m     _leave_task(\u001B[38;5;28mself\u001B[39m._loop, \u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:316\u001B[39m, in \u001B[36mTask.__step_run_and_handle_result\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    314\u001B[39m         result = coro.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    315\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m         result = \u001B[43mcoro\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._must_cancel:\n\u001B[32m    319\u001B[39m         \u001B[38;5;66;03m# Task is cancelled right before coro stops.\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 24\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     21\u001B[39m select_pages = \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;66;03m## 전체는 None, 특정 페이지는 int 또는 list(int) 페이지 번호 (1부터 시작)\u001B[39;00m\n\u001B[32m     23\u001B[39m output_dir = \u001B[33m\"\u001B[39m\u001B[33m./documents\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;66;03m## 통합된 마크다운 파일을 저장할 디렉토리\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m result = \u001B[38;5;28;01mawait\u001B[39;00m zerox(\n\u001B[32m     25\u001B[39m     file_path=file_path,\n\u001B[32m     26\u001B[39m     model=model,\n\u001B[32m     27\u001B[39m     output_dir=output_dir,\n\u001B[32m     28\u001B[39m     custom_system_prompt=custom_system_prompt,\n\u001B[32m     29\u001B[39m     select_pages=select_pages,\n\u001B[32m     30\u001B[39m     **kwargs\n\u001B[32m     31\u001B[39m )\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/core/zerox.py:149\u001B[39m, in \u001B[36mzerox\u001B[39m\u001B[34m(cleanup, concurrency, file_path, maintain_format, model, output_dir, temp_dir, custom_system_prompt, select_pages, **kwargs)\u001B[39m\n\u001B[32m    147\u001B[39m             aggregated_markdown.append(result)\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m149\u001B[39m     results = \u001B[38;5;28;01mawait\u001B[39;00m process_pages_in_batches(\n\u001B[32m    150\u001B[39m         images,\n\u001B[32m    151\u001B[39m         concurrency,\n\u001B[32m    152\u001B[39m         vision_model,\n\u001B[32m    153\u001B[39m         temp_directory,\n\u001B[32m    154\u001B[39m         input_token_count,\n\u001B[32m    155\u001B[39m         output_token_count,\n\u001B[32m    156\u001B[39m         prior_page,\n\u001B[32m    157\u001B[39m     )\n\u001B[32m    159\u001B[39m     aggregated_markdown = [result[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m results \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result[\u001B[32m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m)]\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m## add token usage\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/processor/pdf.py:108\u001B[39m, in \u001B[36mprocess_pages_in_batches\u001B[39m\u001B[34m(images, concurrency, model, temp_directory, input_token_count, output_token_count, prior_page)\u001B[39m\n\u001B[32m     94\u001B[39m tasks = [\n\u001B[32m     95\u001B[39m     process_page(\n\u001B[32m     96\u001B[39m         image,\n\u001B[32m   (...)\u001B[39m\u001B[32m    104\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images\n\u001B[32m    105\u001B[39m ]\n\u001B[32m    107\u001B[39m \u001B[38;5;66;03m# Wait for all tasks to complete\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(*tasks)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:385\u001B[39m, in \u001B[36mTask.__wakeup\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m    383\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__wakeup\u001B[39m(\u001B[38;5;28mself\u001B[39m, future):\n\u001B[32m    384\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m385\u001B[39m         \u001B[43mfuture\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    386\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    387\u001B[39m         \u001B[38;5;66;03m# This may also be a cancellation.\u001B[39;00m\n\u001B[32m    388\u001B[39m         \u001B[38;5;28mself\u001B[39m.__step(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001B[39m, in \u001B[36m_patch_asyncio.<locals>.run\u001B[39m\u001B[34m(main, debug)\u001B[39m\n\u001B[32m     28\u001B[39m task = asyncio.ensure_future(main)\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task.done():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:92\u001B[39m, in \u001B[36m_patch_loop.<locals>.run_until_complete\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m     90\u001B[39m     f._log_destroy_pending = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f.done():\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stopping:\n\u001B[32m     94\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:133\u001B[39m, in \u001B[36m_patch_loop.<locals>._run_once\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    130\u001B[39m curr_task = curr_tasks.pop(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m     \u001B[43mhandle\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    135\u001B[39m     \u001B[38;5;66;03m# restore the current task\u001B[39;00m\n\u001B[32m    136\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m curr_task \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py:88\u001B[39m, in \u001B[36mHandle._run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     87\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mSystemExit\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n\u001B[32m     90\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:396\u001B[39m, in \u001B[36mTask.__wakeup\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m    388\u001B[39m     \u001B[38;5;28mself\u001B[39m.__step(exc)\n\u001B[32m    389\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    390\u001B[39m     \u001B[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001B[39;00m\n\u001B[32m    391\u001B[39m     \u001B[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    394\u001B[39m     \u001B[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001B[39;00m\n\u001B[32m    395\u001B[39m     \u001B[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m396\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:303\u001B[39m, in \u001B[36mTask.__step\u001B[39m\u001B[34m(self, exc)\u001B[39m\n\u001B[32m    301\u001B[39m _enter_task(\u001B[38;5;28mself\u001B[39m._loop, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__step_run_and_handle_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    305\u001B[39m     _leave_task(\u001B[38;5;28mself\u001B[39m._loop, \u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:314\u001B[39m, in \u001B[36mTask.__step_run_and_handle_result\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    311\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    312\u001B[39m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[32m    313\u001B[39m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m         result = \u001B[43mcoro\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    315\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    316\u001B[39m         result = coro.throw(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/processor/pdf.py:50\u001B[39m, in \u001B[36mprocess_page\u001B[39m\u001B[34m(image, model, temp_directory, input_token_count, output_token_count, prior_page, semaphore)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m semaphore:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m semaphore:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m process_page(\n\u001B[32m     51\u001B[39m             image,\n\u001B[32m     52\u001B[39m             model,\n\u001B[32m     53\u001B[39m             temp_directory,\n\u001B[32m     54\u001B[39m             input_token_count,\n\u001B[32m     55\u001B[39m             output_token_count,\n\u001B[32m     56\u001B[39m             prior_page,\n\u001B[32m     57\u001B[39m         )\n\u001B[32m     59\u001B[39m image_path = os.path.join(temp_directory, image)\n\u001B[32m     61\u001B[39m \u001B[38;5;66;03m# Get the completion from LiteLLM\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/pyzerox/processor/pdf.py:77\u001B[39m, in \u001B[36mprocess_page\u001B[39m\u001B[34m(image, model, temp_directory, input_token_count, output_token_count, prior_page, semaphore)\u001B[39m\n\u001B[32m     74\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m formatted_markdown, input_token_count, output_token_count, prior_page\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     \u001B[43mlogging\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mMessages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mFAILED_TO_PROCESS_IMAGE\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m Error:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43merror\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     78\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m, input_token_count, output_token_count, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:2183\u001B[39m, in \u001B[36merror\u001B[39m\u001B[34m(msg, *args, **kwargs)\u001B[39m\n\u001B[32m   2181\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(root.handlers) == \u001B[32m0\u001B[39m:\n\u001B[32m   2182\u001B[39m     basicConfig()\n\u001B[32m-> \u001B[39m\u001B[32m2183\u001B[39m \u001B[43mroot\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1568\u001B[39m, in \u001B[36mLogger.error\u001B[39m\u001B[34m(self, msg, *args, **kwargs)\u001B[39m\n\u001B[32m   1559\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1560\u001B[39m \u001B[33;03mLog 'msg % args' with severity 'ERROR'.\u001B[39;00m\n\u001B[32m   1561\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   1565\u001B[39m \u001B[33;03mlogger.error(\"Houston, we have a %s\", \"major problem\", exc_info=True)\u001B[39;00m\n\u001B[32m   1566\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1567\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.isEnabledFor(ERROR):\n\u001B[32m-> \u001B[39m\u001B[32m1568\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_log\u001B[49m\u001B[43m(\u001B[49m\u001B[43mERROR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1684\u001B[39m, in \u001B[36mLogger._log\u001B[39m\u001B[34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001B[39m\n\u001B[32m   1681\u001B[39m         exc_info = sys.exc_info()\n\u001B[32m   1682\u001B[39m record = \u001B[38;5;28mself\u001B[39m.makeRecord(\u001B[38;5;28mself\u001B[39m.name, level, fn, lno, msg, args,\n\u001B[32m   1683\u001B[39m                          exc_info, func, extra, sinfo)\n\u001B[32m-> \u001B[39m\u001B[32m1684\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1700\u001B[39m, in \u001B[36mLogger.handle\u001B[39m\u001B[34m(self, record)\u001B[39m\n\u001B[32m   1698\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_record, LogRecord):\n\u001B[32m   1699\u001B[39m     record = maybe_record\n\u001B[32m-> \u001B[39m\u001B[32m1700\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcallHandlers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1762\u001B[39m, in \u001B[36mLogger.callHandlers\u001B[39m\u001B[34m(self, record)\u001B[39m\n\u001B[32m   1760\u001B[39m     found = found + \u001B[32m1\u001B[39m\n\u001B[32m   1761\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m record.levelno >= hdlr.level:\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m         \u001B[43mhdlr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m c.propagate:\n\u001B[32m   1764\u001B[39m     c = \u001B[38;5;28;01mNone\u001B[39;00m    \u001B[38;5;66;03m#break out\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1028\u001B[39m, in \u001B[36mHandler.handle\u001B[39m\u001B[34m(self, record)\u001B[39m\n\u001B[32m   1026\u001B[39m \u001B[38;5;28mself\u001B[39m.acquire()\n\u001B[32m   1027\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1028\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43memit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1029\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1030\u001B[39m     \u001B[38;5;28mself\u001B[39m.release()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1164\u001B[39m, in \u001B[36mStreamHandler.emit\u001B[39m\u001B[34m(self, record)\u001B[39m\n\u001B[32m   1162\u001B[39m     \u001B[38;5;66;03m# issue 35046: merged two stream.writes into one.\u001B[39;00m\n\u001B[32m   1163\u001B[39m     stream.write(msg + \u001B[38;5;28mself\u001B[39m.terminator)\n\u001B[32m-> \u001B[39m\u001B[32m1164\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRecursionError\u001B[39;00m:  \u001B[38;5;66;03m# See issue 36272\u001B[39;00m\n\u001B[32m   1166\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/logging/__init__.py:1144\u001B[39m, in \u001B[36mStreamHandler.flush\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1142\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1143\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.stream, \u001B[33m\"\u001B[39m\u001B[33mflush\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1144\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1145\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1146\u001B[39m     \u001B[38;5;28mself\u001B[39m.release()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/lecture-study/52/inflearn-langgraph-agent/.venv/lib/python3.12/site-packages/ipykernel/iostream.py:609\u001B[39m, in \u001B[36mOutStream.flush\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    607\u001B[39m     \u001B[38;5;28mself\u001B[39m.pub_thread.schedule(evt.set)\n\u001B[32m    608\u001B[39m     \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m609\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mevt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mflush_timeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    610\u001B[39m         \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[32m    611\u001B[39m         \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n\u001B[32m    612\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mIOStream.flush timed out\u001B[39m\u001B[33m\"\u001B[39m, file=sys.__stderr__)\n\u001B[32m    613\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/threading.py:655\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    653\u001B[39m signaled = \u001B[38;5;28mself\u001B[39m._flag\n\u001B[32m    654\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[32m--> \u001B[39m\u001B[32m655\u001B[39m     signaled = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cond\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/threading.py:359\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    357\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m359\u001B[39m         gotit = \u001B[43mwaiter\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    360\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    361\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:18:37.300461Z",
     "start_time": "2025-10-05T06:18:37.297383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 100,\n",
    "    separators=['\\n\\n', '\\n']\n",
    ")"
   ],
   "id": "5de6f9f71cfcb555",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:19:07.312986Z",
     "start_time": "2025-10-05T06:18:59.209324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_path = \"./documents/income_tax.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "document_list = loader.load_and_split(text_splitter)"
   ],
   "id": "79b10a4267a5c0c9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "마크다운 로더를 그대로 사용하면 표가 잘릴 수 있다.\n",
    "\n",
    "> 표가 실제 표가 아니라 단순한 텍스트이므로\n",
    "\n",
    "이럴 때는 마크다운 -> txt로 변환 -> load -> split"
   ],
   "id": "7394a9ac33f7bf86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:22:31.489931Z",
     "start_time": "2025-10-05T06:22:31.391796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text_path = './documents/income_tax.txt'\n",
    "\n",
    "# 마크다운 파일을 읽어옵니다\n",
    "with open(markdown_path, 'r', encoding='utf-8') as md_file:\n",
    "    md_content = md_file.read()\n",
    "\n",
    "# 마크다운 콘텐츠를 HTML로 변환합니다\n",
    "html_content = markdown.markdown(md_content)\n",
    "\n",
    "# HTML 콘텐츠를 파싱하여 텍스트만 추출합니다\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "text_content = soup.get_text()\n",
    "\n",
    "# 추출한 텍스트를 텍스트 파일로 저장합니다\n",
    "with open(text_path, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(text_content)\n",
    "\n",
    "print(\"Markdown converted to plain text successfully!\")"
   ],
   "id": "a6bdeef1530d519a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown converted to plain text successfully!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:23:27.914322Z",
     "start_time": "2025-10-05T06:23:27.907981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(text_path)\n",
    "document_list = loader.load_and_split(text_splitter)"
   ],
   "id": "743b6f2602dc6a29",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:27:55.841609Z",
     "start_time": "2025-10-05T06:27:55.733028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings = UpstageEmbeddings(model='solar-embedding-1-large')"
   ],
   "id": "6eb31a64ed8980c5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:29:38.060754Z",
     "start_time": "2025-10-05T06:28:32.740484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document_list,\n",
    "    embedding=embeddings,\n",
    "    collection_name = 'income_tax_collection',\n",
    "    persist_directory = './income_tax_collection'\n",
    ")"
   ],
   "id": "ddddbfb1b664dce2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:29:45.242389Z",
     "start_time": "2025-10-05T06:29:45.240478Z"
    }
   },
   "cell_type": "code",
   "source": "retriever = vector_store.as_retriever(search_kwargs={'k':3})",
   "id": "5bbc6151e7c04d38",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:30:00.120249Z",
     "start_time": "2025-10-05T06:30:00.118720Z"
    }
   },
   "cell_type": "code",
   "source": "query = '연봉 5천만원 직장인의 소득세는?'",
   "id": "b46b4d2fa23a0b38",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:30:07.699745Z",
     "start_time": "2025-10-05T06:30:06.106151Z"
    }
   },
   "cell_type": "code",
   "source": "retriever.invoke(query)",
   "id": "97414e05f37b4ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8ffc8a86-a526-43bf-b74b-3d14634fbcda', metadata={'source': './documents/income_tax.txt'}, page_content='130만원 이하   \\n산출세액의 10분의 55 \\n130만원 초과   \\n71천원과 (130만원을 초과하는 금액의 10분의 30)  \\n\\n② 제1항에 대하여 공제받는 다음 각 호의 계산에 따른 금액을 초과하는 경우에 초과하는 금액은 없는 것으로 한다. <신설 2014. 1. 1., 2015. 5. 13., 2022. 12. 31.>\\n1. 총급여액이 3천만원 이하인 경우: 74천원 = (총급여액 - 300만원) × 8/1000. 단, 위 금액은 적은 경우 6천원으로 한다.\\n2. 총급여액이 3천만원 초과 7천만원 이하인 경우: 66천원 = (총급여액 - 7천만원) × 1/2. 단, 위 금액이 50만원보다 적은 경우에는 50만원으로 한다.\\n3. 총급여액이 1억원 미만일 경우에는 경우: 50만원 = (총급여액 - 1억원) × 1/2. 단, 위 금액이 20만원보다 적은 경우에는 20만원으로 한다.\\n4. 일금공로자의 근로소득에 대해 제134조제3항에 대한 원천징수를 하는 경우에는 해도 근로소득에 대한 산출세액의 100세 50에 해당하는 금액은 그 산출세액에 적용된다. <개정 2014. 1. 1.>\\n[전문개정 2021. 1.]  \\n제59조2의2(자산세예고제)\\n① 종합소득에 있는 거주자이기 근로제대상자에 해당하는 자녀(입장 및 위탁아동을 포함하며, 이하 이 조에서 \"공제대상자\"라 한다) 및 소득세법 제57조 이상의 사례에 대해서는 다음 각 호의 구분에 따른 금액을 종합소득세법에 공제받는다. <개정 2015. 5. 13., 2017. 12. 31., 2018. 12. 19., 2019. 12. 31., 2022. 12. 31., 2023. 12. 31.>  \\n\\n1명인 경우: 연 15만원\\n2명인 경우: 연 35만원\\n3명 이상인 경우: 연 35만원과 2명을 초과하는 1명당 연 30만원을 합한 금액\\n③ 삭제: 2017. 12. 19.\\n④ 해당 과세기간에 출산하거나 입양 신고한 경우 각 호의 구분에 따른 금액을 공통적으로 환산특수산출세액에 세액해당. <신설 2015. 5. 13., 2016. 12. 20.>\\n출산하거나 입양 신고한 공제대상자녀가 첫째인 경우: 연 30만원\\n출산하거나 입양 신고한 공제대상자녀가 둘째인 경우: 연 50만원\\n출산하거나 입양 신고한 공제대상자녀가 셋째 이상인 경우: 연 70만원\\n④ 제1항 및 제3항에 따른 공제는 \"자녀세액공제\"라 한다. <신설 2015. 5. 13., 2017. 12. 19.>\\n[본조신설 2014. 1. 1.]\\n[중전 제59조의2는 제59조의3으로 이동 <2014. 1. 1.>]'),\n",
       " Document(id='32004e44-1451-43d3-8b07-a0be10a6c049', metadata={'source': './documents/income_tax.txt'}, page_content='② 제70조제1항, 제70조의2에 따른 제74조에 따라 차례로 할 것이 제70조제1항제2호에 따르며 서류를 제출하여야 한다는 경우에는 기준소득 중 거주자 본인이 된다(분산)과 제70조제2와 제74조에 따른 제료 및 제대법을 포함한다. 단, 차별제표청정인 그 업체를 남겨 제출한 경우로 그에 대하여 아니하다.<개정 2013. 1. 1.>\\n  ③ 제80조에 따른 수익과 관련의 경우에는 기초공제 중 거주자 본인이 된다(분산)과 그에 관한 적지사항을 분명히 한다.\\n[전문개정 2009. 12. 31.]\\n[제목개정 2014. 1. 1.]\\n제54조의2(공동사업에 대한 소득공제 등 특례) 제51조의3 또는 「조세특례제한법」에 따른 소득공제를 적용하거나 제59조의2에 따른 세액감면을 적용하는 경우 제54조제3항에 따라 공동사업자의 소득에 합산과세되는 특별세액거래의 지출․납입․투자 등의 금액이 있을 경우 주된 공동사업자의 소득에 합산과세되는 소득금액에 합산되어 주된 공동사업자의 합산과세세액은 공동사업소득액 또는 공동사업창출세액을 계산할 때 소득공제 또는 세액공제를 받을 수 있다. \\n[개정 2014. 1. 1.]\\n[전문개정 2009. 12. 31.]\\n[제목개정 2014. 1. 1.]\\n제2절 세액의 계산 <개정 2009. 12. 31.>\\n제1관 세율 <개정 2009. 12. 31.>\\n제55조(세율) 거주자의 종합소득에 대한 소득세는 해당 연도의 종합소득과세표준에 다음의 세율을 적용하여 계산한 금액(이하 \"종합소득과세표준세액\"이라 한다)을 그 세액으로 한다. <개정 2014. 1. 1., 2016. 12. 20., 2017. 12. 19., 2020. 12. 29., 2022. 12. 31.>\\n종합소득\\n┌───────────────┐\\n│ 과세표준의 6개 구간 │\\n├───────────────┤\\n│ 1,400만원 이하        │ 84만원 + (1,400만원을 초과하는 금액의 15%)  │\\n│ 1,400만원 초과        │ 84만원 + (5,000만원을 초과하는 금액의 24%)  │\\n│ 8,800만원 이하        │ 624만원 + (5,000만원을 초과하는 금액의 24%)  │\\n│ 8,800만원 초과        │ 1,536만원 + (8,800만원을 초과하는 금액의 35%)  │\\n│ 1.5억원 초과          │ 4,046만원 + (1,500만원을 초과하는 금액의 38%)  │\\n│ 3억원 초과            │ 6,460만원 + (3억원을 초과하는 금액의 40%)  │\\n│ 5억원 초과            │ 14,760만원 + (5억원을 초과하는 금액의 42%)  │\\n│ 10억원 초과           │ 38,406만원 + (10억원을 초과하는 금액의 45%)  │\\n└───────────────┘\\n② 거주자의 퇴직소득에 대한 소득세는 다음 각 호의 순서에 따라 계산한 금액(이하 ‘퇴직소득 산출세액’이라 한다)으로 한다. <개정 2013. 1. 1., 2014. 12. 23.>'),\n",
       " Document(id='a1729e3d-7642-4eb3-8a04-919b1019a897', metadata={'source': './documents/income_tax.txt'}, page_content='[전문개정 2009. 12. 31.]\\n제47조 근로소득증제 - 연금소득증제 및 퇴직소득증제\\n<개정> 2009. 12. 31.  \\n제48조(근로소득증제)\\n근로소득이 있는 거주자에 대해서는 해당 과세기간에 받는 총급여액에 다음의 금액을 공제한다. 다만, 공제액이 2천만원을 초과하는 경우에는 2천만원을 공제한다. <개정> 2012. 1. 1, 2014. 1. 1, 2019. 12. 31.\\n소득세법\\n총급여액 | 공제액\\n--- | ---\\n500만원 이하 | 총 급여액의 100분의 70\\n500만원 초과 1천500만원 이하 | 350만원+(500만원을 초과하는 금액의 100분의 40)\\n1천500만원 초과 4천500만원 이하 | 750만원+(1천500만원을 초과하는 금액의 100분의 15)\\n4천500만원 초과 1억원 이하 | 1천200만원+(4천500만원을 초과하는 금액의 100분의 5)\\n1억원 초과 | 1천475만원+(1억원을 초과하는 금액의 100분의 2)\\n② 일용근로소득에 대한 공제액은 제1항에도 불구하고 1인 15만원으로 한다. <개정 2018. 12. 31.>\\n③ 근로소득이 있는 거주자의 해악 과세기간의 총급여액에 제1항 또는 제2항의 규정에 미달하는 경우에는 그 총급여액을 공제액으로 한다.\\n④ 제1항부터 제3항까지의 규정에 따른 공제를 “근로소득공제”라 한다.\\n⑤ 제1항의 경우에 2인 이상의 근로소득을 받은 사람(일용근로자는 제외한다)에 대하여는 그 근로소득의 합계액을 총급여액으로 하여 제1항에 따른 근로소득공제를 총급여액에서 공제한다. <개정 2010. 12. 27.>\\n[전문 개정 2009. 12. 31.]\\n제47조2(연금소득공제)\\n① 연금소득이 있는 거주자에 대해서는 해당 과세기간에 받은 총연금액(분리과세연금소득)은 제1항에 이어 항에 갈치에 다음 표에 규정된 금액을 공제한다. 다만, 공제액이 900만원을 초과하는 경우에는 900만원을 공제한다. <개정 2013. 1. 1.>\\n총급여액 | 공제액\\n--- | ---\\n350만원 이하 | 총급여액\\n350만원 초과 700만원 이하 | 350만원+(350만원을 초과하는 금액의 100분의 40)\\n700만원 초과 1천만원 이하 | 630만원+(700만원을 초과하는 금액의 100분의 20)\\n1천만원 초과 | 1천100만원+(1천만원을 초과하는 금액의 100분의 10)\\n② 제1항에 따른 공제를 “연금소득공제”라 한다. [전문 개정 2009. 12. 31.]\\n제48조(퇴직소득공제)\\n① 퇴직소득이 있는 거주자에 대해서는 해당 과세기간의 퇴직소득금액에서 제1호의 구분에 따른 금액을 공제하며, 그 금액은 근속년수(1년 미만이 있는 경우에는 이를 1로 보며, 제22조제1항제1호의 경우에는 대칭범위으로 정하는 방법에 따라 산출한 연수를 판단한다. 이하 같다면 나누고 12를 곱한 후의 금액(이하 이 항에서 “산출근거”라 한다)에 제2호의 구분에 따른 금액을 공제한다. <개정 2014. 12. 23., 2022. 12. 31.>\\n1. 근속연수에 따라 정한 다음의 금액\\n근속연수 | 공제액\\n--- | ---')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:31:27.573690Z",
     "start_time": "2025-10-05T06:31:27.571580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ],
   "id": "f6cdd03cd6f3dac3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:31:43.383310Z",
     "start_time": "2025-10-05T06:31:43.255968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(AgentState)"
   ],
   "id": "53ceb083f6a3dd66",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:40:03.822439Z",
     "start_time": "2025-10-05T06:40:03.819562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자의 질문에 기반하여 벡터 스토어에서 관련 문서를 검색합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 검색된 문서가 추가된 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']  # state에서 사용자의 질문을 추출합니다.\n",
    "    docs = retriever.invoke(query)  # 질문과 관련된 문서를 검색합니다.\n",
    "    return {'context': docs}  # 검색된 문서를 포함한 state를 반환합니다."
   ],
   "id": "5574aa952152f587",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LangChain의 hub를 통해 미리 정의된 RAG 프롬프트를 활용합니다\n",
    "- hub에는 이미 검증된 프롬프트들이 많기 때문에 프로젝트 진행 시 좋은 시작점이 됩니다\n",
    "- hub에서 프롬프트를 찾아보고, 동작을 확인한 후 커스텀 하는 것을 권장합니다"
   ],
   "id": "317c013a744778da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:38:09.959491Z",
     "start_time": "2025-10-05T06:38:09.623580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "from langchain_upstage import ChatUpstage\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatUpstage(model='solar-pro2')"
   ],
   "id": "4ec2b70747046199",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:38:18.409649Z",
     "start_time": "2025-10-05T06:38:18.407618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자의 질문과 검색된 문서를 기반으로 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문과 검색된 문서를 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 생성된 응답이 추가된 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    context = state['context']  # state에서 검색된 문서를 추출합니다.\n",
    "    query = state['query']  # state에서 사용자의 질문을 추출합니다.\n",
    "    rag_chain = prompt | llm  # RAG 프롬프트와 LLM을 연결하여 체인을 만듭니다.\n",
    "    response = rag_chain.invoke({'question': query, 'context': context})  # 질문과 문맥을 사용하여 응답을 생성합니다.\n",
    "    return {'answer': response}  # 생성된 응답을 포함한 state를 반환합니다."
   ],
   "id": "b8a740502bcec018",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:40:07.147627Z",
     "start_time": "2025-10-05T06:40:07.144179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_builder.add_node('retrieve', retrieve)\n",
    "graph_builder.add_node('generate', generate)"
   ],
   "id": "19f01ad2b5daf7e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11156daf0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:40:19.205238Z",
     "start_time": "2025-10-05T06:40:19.203377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder.add_edge(START, 'retrieve')\n",
    "graph_builder.add_edge('retrieve', 'generate')\n",
    "graph_builder.add_edge('generate', END)"
   ],
   "id": "3724fcc89f8441d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11156daf0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:40:24.003758Z",
     "start_time": "2025-10-05T06:40:24.001722Z"
    }
   },
   "cell_type": "code",
   "source": "graph = graph_builder.compile()\n",
   "id": "f7fe5aea49a46ff1",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:40:32.507972Z",
     "start_time": "2025-10-05T06:40:31.797634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "6d90a4b950d85c4f",
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:42:34.693081Z",
     "start_time": "2025-10-05T06:42:31.561343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_state = {'query': query}\n",
    "graph.invoke(initial_state)"
   ],
   "id": "85eeb133f855fbeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '연봉 5천만원 직장인의 소득세는?',\n",
       " 'context': [Document(id='8ffc8a86-a526-43bf-b74b-3d14634fbcda', metadata={'source': './documents/income_tax.txt'}, page_content='130만원 이하   \\n산출세액의 10분의 55 \\n130만원 초과   \\n71천원과 (130만원을 초과하는 금액의 10분의 30)  \\n\\n② 제1항에 대하여 공제받는 다음 각 호의 계산에 따른 금액을 초과하는 경우에 초과하는 금액은 없는 것으로 한다. <신설 2014. 1. 1., 2015. 5. 13., 2022. 12. 31.>\\n1. 총급여액이 3천만원 이하인 경우: 74천원 = (총급여액 - 300만원) × 8/1000. 단, 위 금액은 적은 경우 6천원으로 한다.\\n2. 총급여액이 3천만원 초과 7천만원 이하인 경우: 66천원 = (총급여액 - 7천만원) × 1/2. 단, 위 금액이 50만원보다 적은 경우에는 50만원으로 한다.\\n3. 총급여액이 1억원 미만일 경우에는 경우: 50만원 = (총급여액 - 1억원) × 1/2. 단, 위 금액이 20만원보다 적은 경우에는 20만원으로 한다.\\n4. 일금공로자의 근로소득에 대해 제134조제3항에 대한 원천징수를 하는 경우에는 해도 근로소득에 대한 산출세액의 100세 50에 해당하는 금액은 그 산출세액에 적용된다. <개정 2014. 1. 1.>\\n[전문개정 2021. 1.]  \\n제59조2의2(자산세예고제)\\n① 종합소득에 있는 거주자이기 근로제대상자에 해당하는 자녀(입장 및 위탁아동을 포함하며, 이하 이 조에서 \"공제대상자\"라 한다) 및 소득세법 제57조 이상의 사례에 대해서는 다음 각 호의 구분에 따른 금액을 종합소득세법에 공제받는다. <개정 2015. 5. 13., 2017. 12. 31., 2018. 12. 19., 2019. 12. 31., 2022. 12. 31., 2023. 12. 31.>  \\n\\n1명인 경우: 연 15만원\\n2명인 경우: 연 35만원\\n3명 이상인 경우: 연 35만원과 2명을 초과하는 1명당 연 30만원을 합한 금액\\n③ 삭제: 2017. 12. 19.\\n④ 해당 과세기간에 출산하거나 입양 신고한 경우 각 호의 구분에 따른 금액을 공통적으로 환산특수산출세액에 세액해당. <신설 2015. 5. 13., 2016. 12. 20.>\\n출산하거나 입양 신고한 공제대상자녀가 첫째인 경우: 연 30만원\\n출산하거나 입양 신고한 공제대상자녀가 둘째인 경우: 연 50만원\\n출산하거나 입양 신고한 공제대상자녀가 셋째 이상인 경우: 연 70만원\\n④ 제1항 및 제3항에 따른 공제는 \"자녀세액공제\"라 한다. <신설 2015. 5. 13., 2017. 12. 19.>\\n[본조신설 2014. 1. 1.]\\n[중전 제59조의2는 제59조의3으로 이동 <2014. 1. 1.>]'),\n",
       "  Document(id='32004e44-1451-43d3-8b07-a0be10a6c049', metadata={'source': './documents/income_tax.txt'}, page_content='② 제70조제1항, 제70조의2에 따른 제74조에 따라 차례로 할 것이 제70조제1항제2호에 따르며 서류를 제출하여야 한다는 경우에는 기준소득 중 거주자 본인이 된다(분산)과 제70조제2와 제74조에 따른 제료 및 제대법을 포함한다. 단, 차별제표청정인 그 업체를 남겨 제출한 경우로 그에 대하여 아니하다.<개정 2013. 1. 1.>\\n  ③ 제80조에 따른 수익과 관련의 경우에는 기초공제 중 거주자 본인이 된다(분산)과 그에 관한 적지사항을 분명히 한다.\\n[전문개정 2009. 12. 31.]\\n[제목개정 2014. 1. 1.]\\n제54조의2(공동사업에 대한 소득공제 등 특례) 제51조의3 또는 「조세특례제한법」에 따른 소득공제를 적용하거나 제59조의2에 따른 세액감면을 적용하는 경우 제54조제3항에 따라 공동사업자의 소득에 합산과세되는 특별세액거래의 지출․납입․투자 등의 금액이 있을 경우 주된 공동사업자의 소득에 합산과세되는 소득금액에 합산되어 주된 공동사업자의 합산과세세액은 공동사업소득액 또는 공동사업창출세액을 계산할 때 소득공제 또는 세액공제를 받을 수 있다. \\n[개정 2014. 1. 1.]\\n[전문개정 2009. 12. 31.]\\n[제목개정 2014. 1. 1.]\\n제2절 세액의 계산 <개정 2009. 12. 31.>\\n제1관 세율 <개정 2009. 12. 31.>\\n제55조(세율) 거주자의 종합소득에 대한 소득세는 해당 연도의 종합소득과세표준에 다음의 세율을 적용하여 계산한 금액(이하 \"종합소득과세표준세액\"이라 한다)을 그 세액으로 한다. <개정 2014. 1. 1., 2016. 12. 20., 2017. 12. 19., 2020. 12. 29., 2022. 12. 31.>\\n종합소득\\n┌───────────────┐\\n│ 과세표준의 6개 구간 │\\n├───────────────┤\\n│ 1,400만원 이하        │ 84만원 + (1,400만원을 초과하는 금액의 15%)  │\\n│ 1,400만원 초과        │ 84만원 + (5,000만원을 초과하는 금액의 24%)  │\\n│ 8,800만원 이하        │ 624만원 + (5,000만원을 초과하는 금액의 24%)  │\\n│ 8,800만원 초과        │ 1,536만원 + (8,800만원을 초과하는 금액의 35%)  │\\n│ 1.5억원 초과          │ 4,046만원 + (1,500만원을 초과하는 금액의 38%)  │\\n│ 3억원 초과            │ 6,460만원 + (3억원을 초과하는 금액의 40%)  │\\n│ 5억원 초과            │ 14,760만원 + (5억원을 초과하는 금액의 42%)  │\\n│ 10억원 초과           │ 38,406만원 + (10억원을 초과하는 금액의 45%)  │\\n└───────────────┘\\n② 거주자의 퇴직소득에 대한 소득세는 다음 각 호의 순서에 따라 계산한 금액(이하 ‘퇴직소득 산출세액’이라 한다)으로 한다. <개정 2013. 1. 1., 2014. 12. 23.>'),\n",
       "  Document(id='a1729e3d-7642-4eb3-8a04-919b1019a897', metadata={'source': './documents/income_tax.txt'}, page_content='[전문개정 2009. 12. 31.]\\n제47조 근로소득증제 - 연금소득증제 및 퇴직소득증제\\n<개정> 2009. 12. 31.  \\n제48조(근로소득증제)\\n근로소득이 있는 거주자에 대해서는 해당 과세기간에 받는 총급여액에 다음의 금액을 공제한다. 다만, 공제액이 2천만원을 초과하는 경우에는 2천만원을 공제한다. <개정> 2012. 1. 1, 2014. 1. 1, 2019. 12. 31.\\n소득세법\\n총급여액 | 공제액\\n--- | ---\\n500만원 이하 | 총 급여액의 100분의 70\\n500만원 초과 1천500만원 이하 | 350만원+(500만원을 초과하는 금액의 100분의 40)\\n1천500만원 초과 4천500만원 이하 | 750만원+(1천500만원을 초과하는 금액의 100분의 15)\\n4천500만원 초과 1억원 이하 | 1천200만원+(4천500만원을 초과하는 금액의 100분의 5)\\n1억원 초과 | 1천475만원+(1억원을 초과하는 금액의 100분의 2)\\n② 일용근로소득에 대한 공제액은 제1항에도 불구하고 1인 15만원으로 한다. <개정 2018. 12. 31.>\\n③ 근로소득이 있는 거주자의 해악 과세기간의 총급여액에 제1항 또는 제2항의 규정에 미달하는 경우에는 그 총급여액을 공제액으로 한다.\\n④ 제1항부터 제3항까지의 규정에 따른 공제를 “근로소득공제”라 한다.\\n⑤ 제1항의 경우에 2인 이상의 근로소득을 받은 사람(일용근로자는 제외한다)에 대하여는 그 근로소득의 합계액을 총급여액으로 하여 제1항에 따른 근로소득공제를 총급여액에서 공제한다. <개정 2010. 12. 27.>\\n[전문 개정 2009. 12. 31.]\\n제47조2(연금소득공제)\\n① 연금소득이 있는 거주자에 대해서는 해당 과세기간에 받은 총연금액(분리과세연금소득)은 제1항에 이어 항에 갈치에 다음 표에 규정된 금액을 공제한다. 다만, 공제액이 900만원을 초과하는 경우에는 900만원을 공제한다. <개정 2013. 1. 1.>\\n총급여액 | 공제액\\n--- | ---\\n350만원 이하 | 총급여액\\n350만원 초과 700만원 이하 | 350만원+(350만원을 초과하는 금액의 100분의 40)\\n700만원 초과 1천만원 이하 | 630만원+(700만원을 초과하는 금액의 100분의 20)\\n1천만원 초과 | 1천100만원+(1천만원을 초과하는 금액의 100분의 10)\\n② 제1항에 따른 공제를 “연금소득공제”라 한다. [전문 개정 2009. 12. 31.]\\n제48조(퇴직소득공제)\\n① 퇴직소득이 있는 거주자에 대해서는 해당 과세기간의 퇴직소득금액에서 제1호의 구분에 따른 금액을 공제하며, 그 금액은 근속년수(1년 미만이 있는 경우에는 이를 1로 보며, 제22조제1항제1호의 경우에는 대칭범위으로 정하는 방법에 따라 산출한 연수를 판단한다. 이하 같다면 나누고 12를 곱한 후의 금액(이하 이 항에서 “산출근거”라 한다)에 제2호의 구분에 따른 금액을 공제한다. <개정 2014. 12. 23., 2022. 12. 31.>\\n1. 근속연수에 따라 정한 다음의 금액\\n근속연수 | 공제액\\n--- | ---')],\n",
       " 'answer': AIMessage(content='연봉 5천만원의 경우 근로소득공제 후 과세표준은 약 3,800만원입니다. 세율은 1,400만원 이하 6%, 초과분은 15%로 적용되며, 종합소득세는 약 400만원~500만원 예상됩니다. 정확한 금액은 추가 공제와 세율 구간 계산에 따라 달라질 수 있습니다. (참고: 근로소득공제 및 세율표 기준)  \\n\\n(Answer is based on the provided documents, but exact calculation requires more detailed information.)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 2750, 'total_tokens': 2856, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'solar-pro2-250909', 'system_fingerprint': None, 'id': '9362d092-e6a6-4702-a3a4-00ac48b53364', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--93c49b79-983b-454b-95c6-2c7483d7ebe7-0', usage_metadata={'input_tokens': 2750, 'output_tokens': 106, 'total_tokens': 2856, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9d64fe8fc099e4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
